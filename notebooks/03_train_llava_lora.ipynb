{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "# Train LLaVA with LoRA for Icon Generation\n",
    "\n",
    "This notebook fine-tunes LLaVA-1.5 with LoRA for icon captioning/generation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from datasets import Dataset\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    LlavaForConditionalGeneration,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_NAME = \"llava-hf/llava-1.5-7b-hf\"\n",
    "DATA_DIR = Path(\"../data/icons_256\")\n",
    "META_PATH = Path(\"../data/icons_metadata.jsonl\")\n",
    "OUTPUT_DIR = \"./llava-lora-icongen\"\n",
    "\n",
    "# LoRA Config\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.05\n",
    "\n",
    "# Training Config\n",
    "BATCH_SIZE = 4\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "NUM_EPOCHS = 3\n",
    "LEARNING_RATE = 2e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and processor\n",
    "print(\"Loading model and processor...\")\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME)\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup_lora",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "print(\"\\nLoRA setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "print(\"Loading dataset...\")\n",
    "data = []\n",
    "with open(META_PATH, 'r') as f:\n",
    "    for line in f:\n",
    "        entry = json.loads(line)\n",
    "        img_path = DATA_DIR / entry['image']\n",
    "        if img_path.exists():\n",
    "            data.append({\n",
    "                'image_path': str(img_path),\n",
    "                'caption': entry['caption']\n",
    "            })\n",
    "\n",
    "print(f\"Loaded {len(data)} samples\")\n",
    "dataset = Dataset.from_list(data)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_collator",
   "metadata": {},
   "outputs": [],
   "source": "# Custom Data Collator for LLaVA\n@dataclass\nclass LlavaDataCollator:\n    processor: Any\n    \n    def __call__(self, examples: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n        images = [Image.open(ex['image_path']).convert('RGB') for ex in examples]\n        \n        # Keep captions short to avoid truncation issues\n        # LLaVA uses 576 image tokens, so we need room for those + text\n        prompts = [\n            f\"USER: <image>\\nDescribe this icon in detail.\\nASSISTANT: {ex['caption'][:200]}\"\n            for ex in examples\n        ]\n        \n        batch = self.processor(\n            text=prompts,\n            images=images,\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=False,  # Disable truncation to preserve image tokens\n        )\n        \n        batch['labels'] = batch['input_ids'].clone()\n        return batch\n\ndata_collator = LlavaDataCollator(processor=processor)\nprint(\"Data collator created!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training_args",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_pin_memory=False,\n",
    "    warmup_steps=50,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "print(\"Training arguments configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trainer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Trainer (NO tokenizer parameter!)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "print(\"Trainer created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"Starting training...\\n\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "trainer.save_model(OUTPUT_DIR + \"/final\")\n",
    "processor.save_pretrained(OUTPUT_DIR + \"/final\")\n",
    "print(f\"Model saved to {OUTPUT_DIR}/final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference\n",
    "test_image = Image.open(dataset[0]['image_path'])\n",
    "prompt = \"USER: <image>\\nDescribe this icon in detail.\\nASSISTANT:\"\n",
    "inputs = processor(text=prompt, images=test_image, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, max_new_tokens=100)\n",
    "    \n",
    "print(processor.decode(output[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}